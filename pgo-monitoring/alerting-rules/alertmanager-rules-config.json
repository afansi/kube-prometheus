{"groups":[{"name":"alert-rules","rules":[{"alert":"PGExporterScrapeError","annotations":{"summary":"Postgres Exporter running on {{ $labels.job }} (instance: {{ $labels.instance }}) is encountering scrape errors processing queries. Error count: ( {{ $value }} )"},"expr":"pg_exporter_last_scrape_error \u003e 0","for":"60s","labels":{"service":"postgresql","severity":"critical","severity_num":"300"}},{"alert":"ExporterDown","annotations":{"description":"Metrics exporter service for {{ $labels.job }} running on {{ $labels.instance }} has been down at least 50% of the time for the last 5 minutes. Service may be flapping or down.","summary":"Prometheus Exporter Service Down"},"expr":"avg_over_time(up[5m]) \u003c 0.5","for":"10s","labels":{"service":"system","severity":"critical","severity_num":"300"}},{"alert":"PGIsUp","annotations":{"summary":"postgres_exporter running on {{ $labels.job }} is unable to communicate with the configured database"},"expr":"pg_up \u003c 1","for":"60s","labels":{"service":"postgresql","severity":"critical","severity_num":"300"}},{"alert":"PGIdleTxn","annotations":{"description":"{{ $labels.job }} has at least one session idle in transaction for over 5 minutes.","summary":"PGSQL Instance idle transactions"},"expr":"ccp_connection_stats_max_idle_in_txn_time \u003e 300","for":"60s","labels":{"service":"postgresql","severity":"warning","severity_num":"200"}},{"alert":"PGIdleTxn","annotations":{"description":"{{ $labels.job }} has at least one session idle in transaction for over 15 minutes.","summary":"PGSQL Instance idle transactions"},"expr":"ccp_connection_stats_max_idle_in_txn_time \u003e 900","for":"60s","labels":{"service":"postgresql","severity":"critical","severity_num":"300"}},{"alert":"PGQueryTime","annotations":{"description":"{{ $labels.job }} has at least one query running for over 12 hours.","summary":"PGSQL Max Query Runtime"},"expr":"ccp_connection_stats_max_query_time \u003e 43200","for":"60s","labels":{"service":"postgresql","severity":"warning","severity_num":"200"}},{"alert":"PGQueryTime","annotations":{"description":"{{ $labels.job }} has at least one query running for over 1 day.","summary":"PGSQL Max Query Runtime"},"expr":"ccp_connection_stats_max_query_time \u003e 86400","for":"60s","labels":{"service":"postgresql","severity":"critical","severity_num":"300"}},{"alert":"PGConnPerc","annotations":{"description":"{{ $labels.job }} is using 75% or more of available connections ({{ $value }}%)","summary":"PGSQL Instance connections"},"expr":"100 * (ccp_connection_stats_total / ccp_connection_stats_max_connections) \u003e 75","for":"60s","labels":{"service":"postgresql","severity":"warning","severity_num":"200"}},{"alert":"PGConnPerc","annotations":{"description":"{{ $labels.job }} is using 90% or more of available connections ({{ $value }}%)","summary":"PGSQL Instance connections"},"expr":"100 * (ccp_connection_stats_total / ccp_connection_stats_max_connections) \u003e 90","for":"60s","labels":{"service":"postgresql","severity":"critical","severity_num":"300"}},{"alert":"DiskFillPredict","annotations":{"description":"Disk on {{ $labels.pg_cluster }}:{{ $labels.kubernetes_pod_name }} is predicted to fill in 24 hrs based on current usage","summary":"Disk predicted to be full in 24 hours"},"expr":"predict_linear(ccp_nodemx_data_disk_available_bytes{mount_point!~\"tmpfs\"}[1h], 24 * 3600) \u003c 0 and 100 * ((ccp_nodemx_data_disk_total_bytes - ccp_nodemx_data_disk_available_bytes) / ccp_nodemx_data_disk_total_bytes) \u003e 70","for":"5m","labels":{"service":"postgresql","severity":"warning","severity_num":"200"}},{"alert":"PGClusterRoleChange","annotations":{"summary":"{{ $labels.pg_cluster }} has had a switchover/failover event. Please check this cluster for more details"},"expr":"count by (pg_cluster) (ccp_is_in_recovery_status != ignoring(instance,ip,pod,role) (ccp_is_in_recovery_status offset 5m)) \u003e= 1","for":"60s","labels":{"service":"postgresql","severity":"critical","severity_num":"300"}},{"alert":"PGDiskSize","annotations":{"description":"PGSQL Instance {{ $labels.deployment }} over 75% disk usage at mount point \"{{ $labels.mount_point }}\": {{ $value }}%","summary":"PGSQL Instance usage warning"},"expr":"100 * ((ccp_nodemx_data_disk_total_bytes - ccp_nodemx_data_disk_available_bytes) / ccp_nodemx_data_disk_total_bytes) \u003e 75","for":"60s","labels":{"service":"postgresql","severity":"warning","severity_num":"200"}},{"alert":"PGDiskSize","annotations":{"description":"PGSQL Instance {{ $labels.deployment }} over 90% disk usage at mount point \"{{ $labels.mount_point }}\": {{ $value }}%","summary":"PGSQL Instance size critical"},"expr":"100 * ((ccp_nodemx_data_disk_total_bytes - ccp_nodemx_data_disk_available_bytes) / ccp_nodemx_data_disk_total_bytes) \u003e 90","for":"60s","labels":{"service":"postgresql","severity":"critical","severity_num":"300"}},{"alert":"PGReplicationByteLag","annotations":{"description":"PGSQL Instance {{ $labels.job }} has at least one replica lagging over 50MB behind.","summary":"PGSQL Instance replica lag warning"},"expr":"ccp_replication_lag_size_bytes \u003e 5.24288e+07","for":"60s","labels":{"service":"postgresql","severity":"warning","severity_num":"200"}},{"alert":"PGReplicationByteLag","annotations":{"description":"PGSQL Instance {{ $labels.job }} has at least one replica lagging over 100MB behind.","summary":"PGSQL Instance replica lag warning"},"expr":"ccp_replication_lag_size_bytes \u003e 1.048576e+08","for":"60s","labels":{"service":"postgresql","severity":"critical","severity_num":"300"}},{"alert":"PGReplicationSlotsInactive","annotations":{"description":"PGSQL Instance {{ $labels.job }} has one or more inactive replication slots","summary":"PGSQL Instance inactive replication slot"},"expr":"ccp_replication_slots_active == 0","for":"60s","labels":{"service":"postgresql","severity":"critical","severity_num":"300"}},{"alert":"PGXIDWraparound","annotations":{"description":"PGSQL Instance {{ $labels.job }} is over 50% towards transaction id wraparound.","summary":"PGSQL Instance {{ $labels.job }} transaction id wraparound imminent"},"expr":"ccp_transaction_wraparound_percent_towards_wraparound \u003e 50","for":"60s","labels":{"service":"postgresql","severity":"warning","severity_num":"200"}},{"alert":"PGXIDWraparound","annotations":{"description":"PGSQL Instance {{ $labels.job }} is over 75% towards transaction id wraparound.","summary":"PGSQL Instance transaction id wraparound imminent"},"expr":"ccp_transaction_wraparound_percent_towards_wraparound \u003e 75","for":"60s","labels":{"service":"postgresql","severity":"critical","severity_num":"300"}},{"alert":"PGEmergencyVacuum","annotations":{"description":"PGSQL Instance {{ $labels.job }} is over 110% beyond autovacuum_freeze_max_age value. Autovacuum may need tuning to better keep up.","summary":"PGSQL Instance emergency vacuum imminent"},"expr":"ccp_transaction_wraparound_percent_towards_emergency_autovac \u003e 110","for":"60s","labels":{"service":"postgresql","severity":"warning","severity_num":"200"}},{"alert":"PGEmergencyVacuum","annotations":{"description":"PGSQL Instance {{ $labels.job }} is over 125% beyond autovacuum_freeze_max_age value. Autovacuum needs tuning to better keep up.","summary":"PGSQL Instance emergency vacuum imminent"},"expr":"ccp_transaction_wraparound_percent_towards_emergency_autovac \u003e 125","for":"60s","labels":{"service":"postgresql","severity":"critical","severity_num":"300"}},{"alert":"PGArchiveCommandStatus","annotations":{"description":"PGSQL Instance {{ $labels.job }} has a recent failing archive command","summary":"Seconds since the last recorded failure of the archive_command"},"expr":"ccp_archive_command_status_seconds_since_last_fail \u003e 300","for":"60s","labels":{"service":"postgresql","severity":"critical","severity_num":"300"}},{"alert":"PGSequenceExhaustion","annotations":{"description":"Count of sequences on instance {{ $labels.job }} at over 75% usage: {{ $value }}. Run following query to see full sequence status: SELECT * FROM monitor.sequence_status() WHERE percent \u003e= 75"},"expr":"ccp_sequence_exhaustion_count \u003e 0","for":"60s","labels":{"service":"postgresql","severity":"critical","severity_num":"300"}},{"alert":"PGSettingsPendingRestart","annotations":{"description":"One or more settings in the pg_settings system catalog on system {{ $labels.job }} are in a pending_restart state. Check the system catalog for which settings are pending and review postgresql.conf for changes."},"expr":"ccp_settings_pending_restart_count \u003e 0","for":"60s","labels":{"service":"postgresql","severity":"critical","severity_num":"300"}}]}]}